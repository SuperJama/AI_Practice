# 1.1 数据预处理
## `'='前的均需实例化， .前需加实例化的对象`
## 1.1.1 均值移除
### 通过使用sklearn库的preprocessing包<br>
```
 = preprocessing.scale(data) ///data为需要处理的数据（一般为数组）
  .mean(axis=) ///mean为输出均值，axis为维数
  .std(axis=) ///std为输出标准差
```
## 1.1.2 范围缩放
### 通过使用sklearn库的preprocessing函数<br>
```
data_scaler = preprocessing.MinMaxScaler(feature_range=(0,1)) ///feature_range为缩放范围
data_scaled = data_scaler.fit_transform(data) ///fit_transform为训练数据和转换数据
```
## 1.1.3 归一化
```
 = preprocessing.normalize(data, norm='l1') ///norm将特征向量调整到L1范数
  .fit_transform(data) ///用法与1.1.2相同
```
## 1.1.4 二值化
```
preprocessing.Binarizer(threshold=1.4).transform(data) ///threshold将数据分割开， 此处x<=1.4为0
```
## 1.1.5 独热编码
```
 = preprocessing.OneHotEncoder() 
  .fit(data) ///传入数据 
  .transform([[x,x,x]]).toarray() /// xxx为需要编码的位置，toarray类型转换
```
# 1.2 标记编码方法
### 通过sklearn库的preprocessing包
#### `创建标记`
```
label_encoder = preprocessing.LabelEncoder() 
input_classes = ['audi', 'ford', 'audi', 'toyota', 'ford', 'bmw'] 
label_encoder.fit(input_classes) 
print "\nClass mapping:" 
for i, item in enumerate(label_encoder.classes_): 
    print item, '-->', i
```
#### `反编辑`
```
labels = ['toyota', 'ford', 'audi']
encoded_labels = label_encoder.transform(labels)
print "\nLabels =", labels
print "Encoded labels =", list(encoded_labels)
```
# 1.3 创建线性回归器
## 此处仅介绍相关函数的用法
### open()
```
n(filename, ' ') ///filename为文件地址， ' '为相关操作
```
操作有：'r'; 'rb'; 'r+'; 'rb+'; 'w'; 'wb'; 'w+'; 'wb+'; 'a'; 'ab'; 'a+'; 'ab+'; <br> 
### linear_modal.LinearRegression()(在sklearn库)
```
 = linear_modal.LinearRegression() ///创建线性回归对象
  .fit(X_train, y_train) ///训练数据
.predict(X_test) ///预测
```
# 1.4 计算回归准确性
`平均绝对误差，均方误差，中位数绝对误差越小越好`
`解释方差分，R方得分  越趋近于1越好`
### round()
```
round(data, x)  ///将数据data精确到小数点后两位
```

```
import sklearn.metrics as sm
round(sm.mean_absolute_error(y_test, y_test_pred), 2) ///平均绝对误差
round(sm.mean_squared_error(y_test, y_ test_pred), 2) ///均方误差
round(sm.median_absolute_error(y_ test, y_test_pred),2) ///中位数绝对误差
round(sm.explained_variance_ score(y_test,y_test_pred), 2)  ///解释方差分
round(sm.r2_score(y_test, y_test_pred), 2)  ///R方得分
```
# 1.5 保存模型数据
```
///保存
import pickle
output_model_file = 'saved_model.pkl'
with open(output_model_file, 'w') as f:
        pickle.dump(linear_regressor, f)
///加载
with open(output_model_file, 'r') as f:
    model_linregr = pickle.load(f)
y_test_pred_new = model_linregr.predict(X_test)
```
主要时pickle.dump()和pickle.load()的问题

# 1.6 创建岭回归器
`主要用于减弱对异常值的敏感度`
### linear_modal.Ridge()(在sklearn库)
```
 = linear_model.Ridge(alpha=0.01, fit_intercept=True, max_iter=10000)
 =   .fit(X_train, y_train)
   .predict(X_test)
```
其中，alpha控制回归器复杂度，fit_intercept为是否使用截距，max_iter为最大迭代次数<br>

# 1.7 创建多项式回归器
### PolynomialFeatures()（在sklearn.preprocessing包中）
```
 = PolynomialFeatures(degree=3)
 =   .fit_transform(X_train)
 = linear_model.LinearRegression()
  .fit(X_, y_)
```
degree为多项式的次数，若想预测值接近输出值需要增大degree<br>

# 1.8 其它回归器
## 1.8.1 决策树回归器 + AdaBoost算法
```
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import AdaBoostRegressor
 = DecisionTreeRegressor(max_depth=4)
  .fit(X_train, y_train)
  .predict()
 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),n_estimators=400, random_state=7)
  .fit(X_train, y_train)
  .predict()
```
max_depth为最大深度，n_estimators为最大估计数，random_state为随机种子数<br>
## 1.8.2 随机森林回归器
```
from sklearn.ensemble import RandomForestRegressor
 = RandomForestRegressor(n_estimators=1000, max_depth=10,min_samples_split=1)
  .fit(X_train, y_train)
  .predict(X_test)
```
n_estimators为森林里树的数量，max_depth为最大深度，min_samples_split为拆分内部节点所需的最小样本数<br>
# 1.9 sklearn官网
https://scikit-learn.org/stable/index.html
